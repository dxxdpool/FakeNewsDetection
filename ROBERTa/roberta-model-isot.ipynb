{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9503860,"sourceType":"datasetVersion","datasetId":5784274}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-06T16:02:21.128471Z","iopub.execute_input":"2024-11-06T16:02:21.129400Z","iopub.status.idle":"2024-11-06T16:02:21.499948Z","shell.execute_reply.started":"2024-11-06T16:02:21.129346Z","shell.execute_reply":"2024-11-06T16:02:21.499030Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/isot-dataset/True.csv\n/kaggle/input/isot-dataset/Fake.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install transformers torch pandas scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:02:21.501734Z","iopub.execute_input":"2024-11-06T16:02:21.502403Z","iopub.status.idle":"2024-11-06T16:02:34.025193Z","shell.execute_reply.started":"2024-11-06T16:02:21.502366Z","shell.execute_reply":"2024-11-06T16:02:34.024057Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing the packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm  # For progress bar\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Set GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:02:34.027136Z","iopub.execute_input":"2024-11-06T16:02:34.027439Z","iopub.status.idle":"2024-11-06T16:02:39.566578Z","shell.execute_reply.started":"2024-11-06T16:02:34.027407Z","shell.execute_reply":"2024-11-06T16:02:39.565556Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing the ISOT dataset and combining them","metadata":{}},{"cell_type":"code","source":"true = pd.read_csv('/kaggle/input/isot-dataset/True.csv')\nfake = pd.read_csv('/kaggle/input/isot-dataset/Fake.csv')\n\n# Add labels: 1 for fake, 0 for true\ntrue['label'] = 0\nfake['label'] = 1\n\n# Combine datasets\ndataset = pd.concat([true, fake], ignore_index=True)\n\ndataset['text'] = dataset['title'] + \" \" + dataset['text']\n\n# Drop unnecessary columns\ndataset = dataset[['text', 'label']]\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:02:39.567734Z","iopub.execute_input":"2024-11-06T16:02:39.568223Z","iopub.status.idle":"2024-11-06T16:02:41.992917Z","shell.execute_reply.started":"2024-11-06T16:02:39.568188Z","shell.execute_reply":"2024-11-06T16:02:41.991952Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n0      As U.S. budget fight looms, Republicans flip t...      0\n1      U.S. military to accept transgender recruits o...      0\n2      Senior U.S. Republican senator: 'Let Mr. Muell...      0\n3      FBI Russia probe helped by Australian diplomat...      0\n4      Trump wants Postal Service to charge 'much mor...      0\n...                                                  ...    ...\n44893  McPain: John McCain Furious That Iran Treated ...      1\n44894  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...      1\n44895  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...      1\n44896  How to Blow $700 Million: Al Jazeera America F...      1\n44897  10 U.S. Navy Sailors Held by Iranian Military ...      1\n\n[44898 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>As U.S. budget fight looms, Republicans flip t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U.S. military to accept transgender recruits o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FBI Russia probe helped by Australian diplomat...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Trump wants Postal Service to charge 'much mor...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44893</th>\n      <td>McPain: John McCain Furious That Iran Treated ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44894</th>\n      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44895</th>\n      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44896</th>\n      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44897</th>\n      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>44898 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{"execution":{"iopub.status.busy":"2024-11-06T15:43:42.122283Z","iopub.execute_input":"2024-11-06T15:43:42.122660Z","iopub.status.idle":"2024-11-06T15:43:42.126884Z","shell.execute_reply.started":"2024-11-06T15:43:42.122625Z","shell.execute_reply":"2024-11-06T15:43:42.125822Z"}}},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n\nMAX_LEN = 256\n\ninput_ids_list = []\nattention_masks_list = []\n\nfor text in tqdm(dataset['text'].values, desc=\"Tokenizing\", unit=\"text\"):\n    # Clear the GPU cache to free up memory\n    torch.cuda.empty_cache()\n    \n    # Run garbage collection\n#     gc.collect()\n    \n    encoded = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=MAX_LEN,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n    \n    # Append results to lists\n    input_ids_list.append(encoded['input_ids'])\n    attention_masks_list.append(encoded['attention_mask'])\n\n# Convert lists to tensors and move to GPU if available\ninput_ids = torch.cat(input_ids_list).to(device)\nattention_masks = torch.cat(attention_masks_list).to(device)\nlabels = torch.tensor(dataset['label'].values).to(device)\n\nprint(\"Tokenization complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:02:41.995527Z","iopub.execute_input":"2024-11-06T16:02:41.996141Z","iopub.status.idle":"2024-11-06T16:06:25.846088Z","shell.execute_reply.started":"2024-11-06T16:02:41.996075Z","shell.execute_reply":"2024-11-06T16:06:25.845192Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b4ad359eac342ad9cbd72e6edb3208c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9b54e8b5e8461398fa939329ce08a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3809c7dddcd946eb8f707de0f7fb5024"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6c009150714082a71d3ace8b057339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6edc978bf5374893b2a4f2d4c768fa9c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nTokenizing: 100%|██████████| 44898/44898 [03:42<00:00, 201.91text/s]\n","output_type":"stream"},{"name":"stdout","text":"Tokenization complete!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train and Test split","metadata":{}},{"cell_type":"code","source":"train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.2, random_state=42)\ntrain_masks, val_masks, _, _ = train_test_split(attention_masks, labels, test_size=0.2, random_state=42)\n\n# Create DataLoader\nbatch_size = 8\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:25.847460Z","iopub.execute_input":"2024-11-06T16:06:25.847853Z","iopub.status.idle":"2024-11-06T16:06:25.903311Z","shell.execute_reply.started":"2024-11-06T16:06:25.847807Z","shell.execute_reply":"2024-11-06T16:06:25.902502Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Fine tuning for ROBERTa Model","metadata":{}},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:25.904613Z","iopub.execute_input":"2024-11-06T16:06:25.905250Z","iopub.status.idle":"2024-11-06T16:06:25.911319Z","shell.execute_reply.started":"2024-11-06T16:06:25.905215Z","shell.execute_reply":"2024-11-06T16:06:25.910258Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"model = RobertaForSequenceClassification.from_pretrained(\n    'roberta-base', \n    num_labels=2,\n    output_attentions=False,\n    output_hidden_states=False\n)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:25.912378Z","iopub.execute_input":"2024-11-06T16:06:25.912697Z","iopub.status.idle":"2024-11-06T16:06:29.008313Z","shell.execute_reply.started":"2024-11-06T16:06:25.912663Z","shell.execute_reply":"2024-11-06T16:06:29.007403Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd10bc7930804465bd71363cb0a74aaf"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n\nepochs = 3\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:29.009661Z","iopub.execute_input":"2024-11-06T16:06:29.010065Z","iopub.status.idle":"2024-11-06T16:06:29.448605Z","shell.execute_reply.started":"2024-11-06T16:06:29.010021Z","shell.execute_reply":"2024-11-06T16:06:29.447771Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Training and visualization","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score\nimport plotly.graph_objects as go","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:29.449723Z","iopub.execute_input":"2024-11-06T16:06:29.450320Z","iopub.status.idle":"2024-11-06T16:06:29.462971Z","shell.execute_reply.started":"2024-11-06T16:06:29.450273Z","shell.execute_reply":"2024-11-06T16:06:29.462182Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:29.465461Z","iopub.execute_input":"2024-11-06T16:06:29.465753Z","iopub.status.idle":"2024-11-06T16:06:29.469895Z","shell.execute_reply.started":"2024-11-06T16:06:29.465722Z","shell.execute_reply":"2024-11-06T16:06:29.468896Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"scaler = GradScaler()  # For mixed precision\naccumulation_steps = 4  # Number of steps for gradient accumulation\n\nfor epoch in range(epochs):\n    print(f'Epoch {epoch + 1}/{epochs}')\n    model.train()\n\n    total_loss = 0\n    correct_predictions = 0\n    total_train = 0\n\n    # Create a single progress bar\n    pbar = tqdm(total=len(train_dataloader), desc=\"Training\")\n    \n    for step, batch in enumerate(train_dataloader):\n        torch.cuda.empty_cache()\n        \n        batch = tuple(b.to(device) for b in batch)\n        inputs, masks, labels = batch\n        \n        # Enable autocasting for mixed precision\n        with autocast():\n            outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)  # Updated for RoBERTa\n            loss = outputs.loss\n            \n        # Accumulate gradients\n        total_loss += loss.item()\n\n        # Calculate training accuracy\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        correct_predictions += (predictions == labels).sum().item()\n        total_train += labels.size(0)\n\n        # Scale the loss and call backward() to create the gradients\n        scaler.scale(loss).backward()\n        \n        # Update weights after accumulating gradients\n        if (step + 1) % accumulation_steps == 0:\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()  # Clear gradients for the next step\n\n        # Update progress bar\n        pbar.update(1)  # Increment progress bar\n\n    pbar.close()  # Close the progress bar after training\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    avg_train_accuracy = correct_predictions / total_train\n    train_losses.append(avg_train_loss)\n    train_accuracies.append(avg_train_accuracy)\n\n    print(f'Average training loss: {avg_train_loss:.2f}')\n    print(f'Training accuracy: {avg_train_accuracy:.2f}')\n    \n    model.eval()\n    eval_loss = 0\n    correct_predictions_val = 0\n    total_val = 0\n    predictions_all = []\n    true_labels_all = []\n\n    # Evaluate without gradient tracking\n    for batch in val_dataloader:\n        batch = tuple(b.to(device) for b in batch)\n        inputs, masks, labels = batch\n\n        with torch.no_grad():\n            # Pass labels for loss calculation\n            outputs = model(input_ids=inputs, attention_mask=masks, labels=labels) \n\n        logits = outputs.logits\n        eval_loss += outputs.loss.item() \n\n        # Calculate validation accuracy\n        predictions = torch.argmax(logits, dim=-1)\n        correct_predictions_val += (predictions == labels).sum().item()\n        total_val += labels.size(0)\n\n        predictions_all.extend(predictions.cpu().numpy())\n        true_labels_all.extend(labels.cpu().numpy())\n\n    avg_val_loss = eval_loss / len(val_dataloader)\n    avg_val_accuracy = correct_predictions_val / total_val\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(avg_val_accuracy)\n\n    print(f'Validation Loss: {avg_val_loss:.2f}')\n    print(f'Validation Accuracy: {avg_val_accuracy:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:06:29.471077Z","iopub.execute_input":"2024-11-06T16:06:29.471394Z","iopub.status.idle":"2024-11-06T16:41:13.447650Z","shell.execute_reply.started":"2024-11-06T16:06:29.471363Z","shell.execute_reply":"2024-11-06T16:41:13.446486Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/512028132.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()  # For mixed precision\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/4490 [00:00<?, ?it/s]/tmp/ipykernel_30/512028132.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nTraining: 100%|██████████| 4490/4490 [09:30<00:00,  7.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.04\nTraining accuracy: 0.98\nValidation Loss: 0.01\nValidation Accuracy: 1.00\nEpoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4490/4490 [09:32<00:00,  7.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.00\nTraining accuracy: 1.00\nValidation Loss: 0.01\nValidation Accuracy: 1.00\nEpoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 4490/4490 [09:31<00:00,  7.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss: 0.00\nTraining accuracy: 1.00\nValidation Loss: 0.00\nValidation Accuracy: 1.00\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# After the training loop, calculate other metrics\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels_all, predictions_all))\n\n# Calculate ROC-AUC score if the task is binary classification\nroc_auc = roc_auc_score(true_labels_all, predictions_all)\nprint(f'ROC-AUC Score: {roc_auc:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:43:57.069598Z","iopub.execute_input":"2024-11-06T16:43:57.070612Z","iopub.status.idle":"2024-11-06T16:43:57.108778Z","shell.execute_reply.started":"2024-11-06T16:43:57.070560Z","shell.execute_reply":"2024-11-06T16:43:57.107751Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      4330\n           1       1.00      1.00      1.00      4650\n\n    accuracy                           1.00      8980\n   macro avg       1.00      1.00      1.00      8980\nweighted avg       1.00      1.00      1.00      8980\n\nROC-AUC Score: 1.00\n","output_type":"stream"}]},{"cell_type":"code","source":"fig = go.Figure()\n\n# Loss\nfig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=train_losses, mode='lines+markers', name='Training Loss'))\nfig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=val_losses, mode='lines+markers', name='Validation Loss'))\n\n# Accuracy\nfig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=train_accuracies, mode='lines+markers', name='Training Accuracy'))\nfig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=val_accuracies, mode='lines+markers', name='Validation Accuracy'))\n\n# Update layout\nfig.update_layout(\n    title='Training and Validation Loss and Accuracy',\n    xaxis_title='Epochs',\n    yaxis_title='Loss / Accuracy',\n    legend_title='Metrics'\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:43:57.411875Z","iopub.execute_input":"2024-11-06T16:43:57.412288Z","iopub.status.idle":"2024-11-06T16:43:57.834032Z","shell.execute_reply.started":"2024-11-06T16:43:57.412247Z","shell.execute_reply":"2024-11-06T16:43:57.833135Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.32.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"de143928-e0fd-4cd5-8d21-fee6d6da4818\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"de143928-e0fd-4cd5-8d21-fee6d6da4818\")) {                    Plotly.newPlot(                        \"de143928-e0fd-4cd5-8d21-fee6d6da4818\",                        [{\"mode\":\"lines+markers\",\"name\":\"Training Loss\",\"x\":[1,2,3],\"y\":[0.04360205592848941,0.0027428553869039284,0.0009221955304156433],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Validation Loss\",\"x\":[1,2,3],\"y\":[0.01098554217391394,0.009412855307968832,0.004006311496452678],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Training Accuracy\",\"x\":[1,2,3],\"y\":[0.981151511776825,0.9997215880616961,0.9999443176123393],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Validation Accuracy\",\"x\":[1,2,3],\"y\":[0.9993318485523385,0.9994432071269488,0.9997772828507795],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Training and Validation Loss and Accuracy\"},\"xaxis\":{\"title\":{\"text\":\"Epochs\"}},\"yaxis\":{\"title\":{\"text\":\"Loss \\u002f Accuracy\"}},\"legend\":{\"title\":{\"text\":\"Metrics\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('de143928-e0fd-4cd5-8d21-fee6d6da4818');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Save the model for further analysis","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\nsave_directory = \"/kaggle/working/model_output\"\nos.makedirs(save_directory, exist_ok=True)\n\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\n\nzip_file_path = \"/kaggle/working/model_output.zip\"\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T16:45:39.492483Z","iopub.execute_input":"2024-11-06T16:45:39.493243Z","iopub.status.idle":"2024-11-06T16:46:20.843491Z","shell.execute_reply.started":"2024-11-06T16:45:39.493200Z","shell.execute_reply":"2024-11-06T16:46:20.842537Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model_output.zip'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}